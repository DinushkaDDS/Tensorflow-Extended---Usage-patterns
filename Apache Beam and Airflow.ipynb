{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Beam and Airflow for Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've been looking at building ML pipeline componets separately and how they would interconnect. Now we should look at how we can put all those components together and how to run a full pipeline. To do that we will mainly look at 2 orchestors Apache Airflow and Beam.\n",
    "\n",
    "The importance of this type of orchestration tool is that otherwise we would need to write code to check when a component complete its task and trigger the other etc. (earlier we used interactive pipelines, but we cant use those in production systems.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Beam\n",
    "\n",
    "TFX comes with a beam version by default. Therefore for a minimal setup using beam as orchastration tool is a valid choice. It is straight forward to setup and use while allowing to use existing data processing pipelines like google cloud dataflow.\n",
    "But it lacks tools for scheduling model updates or monitoring the pipeline job progresses.\n",
    "\n",
    "\n",
    "### Apache Airflow\n",
    "\n",
    "Apache Airflow is already a widely used tool for data-loading task. It provide support to connect with production ready databases like PostgreSQL and execute partial pipelines etc which can save significant amount of time.\n",
    "\n",
    "\n",
    "### Kubeflow pipelines\n",
    "\n",
    "If there's an already existing kubernetes pipeline then it would make sense to use a Kubeflow pipelines tool. But it is complicated to setup compared to other tools. On the other hand it opens up opportunities to view TFDV and TFMA visializations, model lineage and artifact collections.\n",
    "\n",
    "\n",
    "With those details in mind, we can now look in to converting our interactive code to a script which can be used to automate the whole process.\n",
    "\n",
    "First we will define a function to initialize all the component required by our pipeline. This helps us to during the configuration of pipelines to setup for different orchastrators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For demonstration only. DO NOT RUN!\n",
    "\n",
    "\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx.components import (CsvExampleGen, Evaluator, ExampleValidator, Pusher,\n",
    "                            ResolverNode, SchemaGen, StatisticsGen, Trainer,\n",
    "                            Transform)\n",
    "from tfx.components.base import executor_spec\n",
    "from tfx.components.trainer.executor import GenericExecutor\n",
    "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
    "from tfx.proto import pusher_pb2, trainer_pb2\n",
    "from tfx.types import Channel\n",
    "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
    "\n",
    "\n",
    "def init_components(data_dir, module_file, serving_model_dir, training_steps, eval_steps):\n",
    "\n",
    "\n",
    "    example_gen = CsvExampleGen(...)\n",
    "    statistics_gen = StatisticsGen(...)\n",
    "    schema_gen = SchemaGen(...)\n",
    "    validator = ExampleValidator(...)\n",
    "    transformer = Transform(...)\n",
    "    trainer = Trainer(...)\n",
    "    model_resolver = ResolverNode(...)\n",
    "    eval_config = tfma.EvalConfig(...)\n",
    "    evaluator = Evaluator(...)\n",
    "    pusher = Pusher(...)\n",
    "\n",
    "    components = [\n",
    "        example_gen, statistics_gen, schema_gen, \n",
    "        validator, transformer, trainer, \n",
    "        model_resolver, evaluator, pusher\n",
    "    ]\n",
    "    return components\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this function we need to provide the inputs such as data location, where the model should be saved, module files required by the trainer and transform components along with hyperparameters.\n",
    "\n",
    "\n",
    "When we are converting a simple interactive pipeline to a beam or airflow version we can directly use our jupyter notebooks. For any cells we dont need to export use the cell magic `%%skip_for_export`. Then we can setup the rest as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN! Dummy Code segment\n",
    "\n",
    "orchester_type = 'beam'  # or airflow\n",
    "pipeline_name  = 'test_pipeline_beam'\n",
    "\n",
    "notebook_file = 'notebook/file/location'\n",
    "\n",
    "#pipeline inputs\n",
    "data_dir = 'data/stored/path'\n",
    "module_file = 'module/file/stored/path'\n",
    "requirement_file = 'requirement/config/file/path'\n",
    "\n",
    "output_base_path = 'base/output/path'\n",
    "serving_model_dir = 'model/save/dir/path'\n",
    "pipeline_root = 'pipeline/base/path'\n",
    "metadata_path = 'data/store/path'\n",
    "\n",
    "pipeline_export_file = 'pipline/script/export/path'\n",
    "\n",
    "# From interactive context of TFX run this\n",
    "context.export_to_pipeline(notebook_file_path=notebook_file,\n",
    "                           export_file_path=pipeline_export_file,\n",
    "                           runner_type=orchester_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code will generate a python script that can be run using Apache beam or Airflow directly. Else we can go through the method descriobed below as well.\n",
    "\n",
    "### Orchestrating using Apache Beam\n",
    "\n",
    "Apache beam is not complex nor feature rich as airflow or kubeblow. But it is easy to use and is a good method to troubleshoot/debug our works. \n",
    "\n",
    "Below we have defined a Beam Pipeline which accept the TFX pipeline components as an argument and also connects to the SQLite database as the metadata store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import absl # used for advanced logging. https://abseil.io/docs/python/guides/logging\n",
    "from tfx.orchestration import metadata, pipeline\n",
    "\n",
    "def init_beam_pipe(components, pipeline_root, direct_num_workers):\n",
    "\n",
    "    absl.logging.info(f'Pipeline root is set to {pipeline_root}.')\n",
    "\n",
    "    beam_args = [f'--direct_num_workers={direct_num_workers}', \n",
    "                 f'--requirements_file={requirement_file}']\n",
    "\n",
    "    p = pipeline.Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=components,\n",
    "        enable_cache=False, # If this is true some components will run using cached data\n",
    "        metadata_connection_config=metadata.sqlite_metadata_connection_config(metadata_path),\n",
    "        beam_pipeline_args=beam_args\n",
    "    )\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
    "\n",
    "components = init_components(...)\n",
    "\n",
    "pipeline = init_beam_pipe(components, pipeline_root, direct_num_workers=2)\n",
    "BeamDagRunner().run(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like that we can define a beam pipeline and run. Above is not a concrete code sample. But outline the gist of what needs to be done. Also we can scale up this pipeline using `Apache Flink` if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orchesrating using Airflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Machine_Learning_Pipelines-cCuFzokH')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09d8130ec584c6fad87bda1a68cc74a67224bab37bcf74941c3cce44e422d7e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
