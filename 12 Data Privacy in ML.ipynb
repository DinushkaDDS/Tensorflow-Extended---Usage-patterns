{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Privacy in ML Systems\n",
    "\n",
    "Data privacy and security issues are one of the main concerns have in today world ML field. Active researches are being done to idenitify potential solutions so that data can be used in a privacy preserving ways which include keeping the trust of users, limiting the exposure of data etc. \n",
    "\n",
    "Due to increasing pressure from legal side (which is a good thing IMO) data privacy has become a important factor specially for larger organizations.\n",
    "\n",
    "Below are some techniques/concepts used in today ML world.\n",
    "\n",
    "\n",
    "### **Differential Privacy**\n",
    "\n",
    "Differential privacy gives a mathematical measure of privacy loss, that a person experiences by being included in a dataset. \n",
    "\n",
    "To explain this further, if a ML model trainedwith privacy in mind, then the prediction that model make should not change if one person is removed from the training set. This is achieved by adding noise/randomness to the dataset.\n",
    "\n",
    "To provide a more concrete example for such instance, lets say a case where we need to get the answer 'Have you been convicted of a crime?'. This is a very sensitive question. So we can make it random by fipping a coin. If head came, user will answer truthfully. If tail came, user will flip coin again. This time if head came will answer as 'yes' and otherwise 'no'.\n",
    "\n",
    "In above example like scenarios, person have the ability to deny and since we know the probabilty of coin flip, we can get the general proportions of people related to the asked questions.\n",
    "\n",
    "There are parameters to evaluate privacy named epsilon, delta which indicates the mathematical value for the provided privacy. (I am not particularly interested about this topic, so explanations. But if needed can learn more about this from 'The Algorithmic Foundations of Differential Privacy' book.)\n",
    "\n",
    "> To help with differential privacy  Tensorflow provides `Tensorflow Privacy` package. We can use it as an model training optimizer to add randomness. More details can be found in the respective documentation.\n",
    "\n",
    "\n",
    "### **Federated Learning (FL)**\n",
    "\n",
    "Federated learning is a technique where training a machine learning model is distributed across different devices and the final model is formulated in a central server. The point in doing this is data will never leave to a separated location/server from the originated device during the training process. This provides us with a very different paradigm than collecting data and then training a model in a compute cluster.\n",
    "\n",
    "FL is generally useful in the context of mobile devices and IoT. This technique helps users to not to move data from their devices and still help building a ML model.\n",
    "\n",
    "In FL edge devices will receive a model architecture and set of instructions to perform and once the model trained on the given data, weights will be sent back to the central server. There are several techniques to securely send such data to central location and active researches are being made to make this process more secure. \n",
    "\n",
    "> This is an interesting topic to learn more, if you are interested in machine learning and also data security area.\n",
    "\n",
    "This is an active area of research where many challenges are needed to be solved. Edge devices low computational power, security when weight aggregation, distributed training/optimizations on edge devices are some of such areas.\n",
    "\n",
    "> TFX provides `Tensorflow Federated` as an experimentation for such scenarios. Also there's a framework named `Pysyft` focused on privacy preserving ML. \n",
    "\n",
    "\n",
    "### **Encrypted ML**\n",
    "\n",
    "This is another technique used in privacy preserving machine leaning field. It is mainly based on cryptography in computer security and has been a main attraction in ML privacy field. \n",
    "\n",
    "There are techniques such as homagraphic encryption (HE) and secure multiparty computation (SMPC) to encrypt and can be used in either to encrypt a model that has already been trained or to encrypt the whole system.\n",
    "\n",
    "HE method is similar to that of public key cryptography except data would not be decrypted during the transformations. Instead all the computations will performed on top of the encrypted data and once prediction made it would be sent back to the user. Only the user can decrpt the data/predictions to obtain the result.\n",
    "\n",
    "On the other hand, SMPC allows several parties to combine data and perform computations on it and then see the results on their own data without having a knowledge of others data. This is acheived by a technique named secret sharing, which split a single value to multiple shares and by sending to multiple parties. It has very interesting properties like original value cant be reconstructed by any share, result of computations are meaningless until all shares are combined etc.\n",
    "\n",
    "Encrypted ML is provided by Tensorflow Encrypted module. Anyhow encrypted ML usage is farily easy but have drastic drawbacks. Therefore using in production systems is limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Machine_Learning_Pipelines-cCuFzokH')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09d8130ec584c6fad87bda1a68cc74a67224bab37bcf74941c3cce44e422d7e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
