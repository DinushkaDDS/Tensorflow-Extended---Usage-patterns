{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment using TFX\n",
    "\n",
    "We can develop our models anyway we can. But when it comes to real world applications most important factor is how we are going to use the model and therefore model serving aspects act as a major consideration. \n",
    "\n",
    "ML model can be served in 3 major methods. \n",
    "\n",
    "1. ML inference server (An API or scheduled process)\n",
    "2. User browser (When data is sensitive)\n",
    "3. Edge device  (IoT, Remote sensors etc.)\n",
    "\n",
    "In this notebook, we will consider the first point mainly.\n",
    "\n",
    "When creating a Inference server, we rougly follow the same steps. These include,\n",
    "\n",
    "* Create a web app (Django, Flask, FastAPI)\n",
    "* Create the API endpoint\n",
    "* Load the model weights and parameters\n",
    "* Define the predict function based on the expected input\n",
    "* Return the predictions\n",
    "\n",
    "Example simple server code is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do not run the code, This is for demonstration only.\n",
    "\n",
    "import json\n",
    "from flask import Flask, request\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('my_model_saved_path')\n",
    "app = Flask('app_name')\n",
    "\n",
    "@app.route('/classify', methods=['POST'])\n",
    "def classifiy():\n",
    "    data = request.form['data']\n",
    "    procesed_data = preprocess(data)\n",
    "    prediction = model.predict(procesed_data)\n",
    "    return json.dumps({'preds': prediction})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is great for demonstration. But there are risks associated with such approaches.\n",
    "\n",
    "1. **Lack of code separation** - As we can see data science code and model deployment related(web development) code is now on same module. This is problematic as ML models are frequently change compared to the web application and therefore causes unnecessary dependencies and ownership issues.\n",
    "2. **Lack of Model version control**\n",
    "3. **Inefficiencies in model inference under high load**\n",
    "\n",
    "\n",
    "### Tensorflow Serving\n",
    "\n",
    "Tensorflow serving provides support the functionality to load models from a given source and notifies the loader if the source has changed(version). Everything inside the Tensorflow serving is handled by a Model manager component. Its overall architecture is as below.\n",
    "\n",
    "<center><image src=\"imgs/3.jpg\" width=\"400\"/></center>\n",
    "\n",
    "Model manager will handle the model loading based on the policies defined and will provide a serving handler. Therefore data scientists can provide new model versions and TFS can update automatically once it detects a new version.\n",
    "\n",
    "Before using TF models in TFS, they need to be in certain type.\n",
    "\n",
    "<center>\n",
    "\n",
    "`saved_model_path = model.save(file path=\"./saved_models\", save_format=\"tf\")`\n",
    "</center>\n",
    "\n",
    "Also is is recommended to add timestamp to the model name when we are manually saving the model like above. It helps in reconginize the models later easier.\n",
    "\n",
    "TFS uses a concept called Model signatures to identify model graph inputs, outputs and graph signature. This definition allows us to update the model without changing the requests to the model server since signature helps to identify the mappings between inputs and related graph nodes. More details about this can be found in the here.\n",
    "\n",
    "To install Tensorflow Serving python API use below.\n",
    "\n",
    "<center>\n",
    "\n",
    "`pip install tensorflow-serving-api`\n",
    "</center>\n",
    "\n",
    "This provides a commandline tool which can be used to inspect exported model signatures and test the exported models without deploying.\n",
    "\n",
    "Below are some example for this tool usages.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel contains the following tag-sets:\n",
      "'serve'\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir data/tfx/Trainer/model/6/format-Serving/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our computation graph we may have different tags for CPU based inference, GPU based, Training or Serving. These tags are required to understand the computational graph execution. Once we have the tag we can inspect the model signature like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:\n",
      "SignatureDef key: \"__saved_model_init_op\"\n",
      "SignatureDef key: \"serving_default\"\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir data/tfx/Trainer/model/6/format-Serving/ --tag_set serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above output shows the our model with outlined tag have only one model signature named 'serving_default'. To obtain the detailed details about the model we can use the tag and signature like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['examples'] tensor_info:\n",
      "      dtype: DT_STRING\n",
      "      shape: (-1)\n",
      "      name: serving_default_examples:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['outputs'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 1)\n",
      "      name: StatefulPartitionedCall_11:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir data/tfx/Trainer/model/6/format-Serving/ --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above displays the model signature in terms of its inputs and outputs. Their are several model signature types and more details can be found in TF documentation.\n",
    "\n",
    "Other than above, we can test the model in CLI like below as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --dir data/tfx/Trainer/model/6/format-Serving/ \\\n",
    "                --tag_set serve --signature_def serving_default \\\n",
    "                    --input_examples \"examples=[{'key':'value', ...}]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Tensorflow Serving\n",
    "\n",
    "There are 2 main methods of using TF Serving (technically same!).\n",
    "\n",
    "1. **Docker usage**\n",
    "    - This is the easiest way as we can download the prebuilt docker image. \n",
    "    - `docker pull tensorflow/serving` (for CPU based)\n",
    "    - `docker pull tensorflow/serving:latest-gpu (for GPU based)\n",
    "\n",
    "2. **Native installation**\n",
    "    - If we have dedicated server we can use this method.\n",
    "    - Need to setup the environment explicitly by adding custom packages.\n",
    "\n",
    "\n",
    "Once we are done with setup, we need to configure the Tensorflow server. Out of the box, TF server can run in 2 modes. First we can define the model and serving will always provide the latest model. Else we can define a configuration file with model and versions we want to load and TF server will only serve those.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single model configuration\n",
    "\n",
    "We can make tensorflow run loading a single model and switching to new models whenever they are available. Such configutation is called Single model configuration.\n",
    "\n",
    "In docker environment we can define it like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -p 8500:8500 \\ \n",
    "            -p 8501:8501 \\\n",
    "            --mount type=bind,source=/tmp/models,target=/models/my_model \\   # Mounting Storage to docker container\n",
    "            -e MODEL_NAME=my_model \\     # Model name\n",
    "            -e MODEL_BASE_PATH=/models/my_model \\  # Model location\n",
    "            -t tensorflow/serving # Docker image name (if GPU version change this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2 port are for REST api and google Remote Procedure Call (gRPC) endpoints.\n",
    "\n",
    "If we want to deploy model in native installation (not in docker) then we can use below command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorflow_model_server --port=8500 \\\n",
    "                          --rest_api_port=8501 \\\n",
    "                          --model_name=my_model \\\n",
    "                          --model_base_path=/models/my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main advantage in tensorflow serving is `hot-swappable` capability of models. When a new model is delivered, TFS manager will unload the old model and load the new one automatically. This is also useful in rollback situations based on the model versioning.\n",
    "\n",
    "#### Multi model configuration\n",
    "\n",
    "In this method we configure tensorflow serving to load multiple models at the same time. To do that we need to define a configuation file to specify the models.\n",
    "\n",
    "<pre>\n",
    "model_config_list {\n",
    "  config {\n",
    "    name: 'my_model'\n",
    "    base_path: '/models/my_model/'\n",
    "    model_platform: 'tensorflow'\n",
    "  }\n",
    "  config {\n",
    "    name: 'another_model'\n",
    "    base_path: '/models/another_model/'\n",
    "    model_platform: 'tensorflow'\n",
    "  }\n",
    "}\n",
    "</pre>\n",
    "\n",
    "Also if needed we can define model loading policies like specific versions, tags etc for advance usages (like A/B testing).\n",
    "\n",
    "After defining the model config file, we can load the config files in the tensorflow serve (docker environment) like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -p 8500:8500 \\\n",
    "             -p 8501:8501 \\\n",
    "             --mount type=bind,source=/tmp/models,target=/models/my_model \\\n",
    "             --mount type=bind,source=/tmp/model_config,\\\n",
    "             target=/models/model_config \\ \n",
    "             -e MODEL_NAME=my_model \\\n",
    "             -t tensorflow/serving \\\n",
    "             --model_config_file=/models/model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or in native environment like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorflow_model_server --port=8500 \\\n",
    "                          --rest_api_port=8501 \\\n",
    "                          --model_config_file=/models/model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we setup our model server, We can do our predictions by calling its REST api endpoint (or invoking the RPC). We can use generic python libraries for that.\n",
    "\n",
    "**Calling REST**\n",
    "\n",
    "The generic URL pattern would be something like below.\n",
    "\n",
    "<center>\n",
    "\n",
    "`http://{HOST}:{PORT}/v1/models/{MODEL_NAME}:{VERB}`\n",
    "</center>\n",
    "\n",
    "Here VERB means the type of signature we need use (predict, regress or classify). Also if we need to specify the model version need to use URL pattern would be like below.\n",
    "\n",
    "<center>\n",
    "\n",
    "`http://{HOST}:{PORT}/v1/models/{MODEL_NAME}[/versions/${MODEL_VERSION}]:{VERB}`\n",
    "</center>\n",
    "\n",
    "The request payload would be a simple json data, with either 'instances (for multi examples)' or 'inputs (for single example)' keys (But not both in same request).\n",
    "\n",
    "**Calling gRPC**\n",
    "\n",
    "Invoking RPCs are bit different than calling a REST api. \n",
    "[gRPC Documentation](https://grpc.io/docs/what-is-grpc/introduction/)\n",
    "\n",
    "First we need to establish a gRPC channel. This chennel provides us the connection to the  grpc server at a given host ip and port. Then we will create a stub object. (I dont remember the exact workings of RPC methods, so its better to read about them!)\n",
    "\n",
    "This is bit complex than using just REST apis, but generally more performant as well. Read more about the usage in the [documentation of Tensorflow serving](https://www.tensorflow.org/tfx/serving/api_rest).\n",
    "\n",
    "\n",
    "### A/B Testing with Tensorflow Serving\n",
    "\n",
    "A/B Testing helps us to understand how 2 (or more for that matter) different models will behave in a production setting. But technically TFS does not provide functionality to divert requests to 2 models from server side. Instead we can direct our requests from client side to different models mimicing the random behaviour. But we will need to calculate the statistics manually from the server side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch inferencing using TFS\n",
    "\n",
    "This is one of the very useful feature provided by the TFX framework to properly utilize the computing resources. Batch inference need to be enabled before using and we need to setup few configuration values.\n",
    "\n",
    "- **max_batch_size** => batch size to be collected\n",
    "- **batch_timeout_macros** => maximum wait time for filling a batch\n",
    "- **max_enqueued_batches** => sets the maximum number of batched get queued for prediction. setting this helps to avoid congesion and will return an error to user\n",
    "- **num_batch_threads** => how many CPU/GPU cores can be used in parrallel\n",
    "- **pad_variable_lenght_inputs** => boolean to process input variables to same size\n",
    "\n",
    "Once we decide on the values we need to create a file with related parameters contained like below.\n",
    "\n",
    "<pre>\n",
    "max_batch_size { value: 32 }\n",
    "batch_timeout_micros { value: 5000 }\n",
    "pad_variable_length_inputs: true\n",
    "</pre>\n",
    "\n",
    "Then after saving it, we can rerun the TFS service with 2 additional parameters defining the batch mode enabling and configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -p 8500:8500 \\\n",
    "            -p 8501:8501 \\\n",
    "            --mount type=bind,source=/path/to/models,target=/models/my_model \\\n",
    "            --mount type=bind,source=/path/to/batch_config,target=/server_config \\\n",
    "            -e MODEL_NAME=my_model -t tensorflow/serving \\\n",
    "            --enable_batching=true\n",
    "            --batching_parameters_file=/server_config/batching_parameters.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also there are several other parameter we can optimize in tensorflow serving to provide better inferencing service. Please read the documentation regarding such parameters.\n",
    "\n",
    "> Other than TFS there are few other options to serve ML models. These include `BentoML`, `Seldon`, `GraphPipe`, `MLflow` and `Rayserve`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Machine_Learning_Pipelines-cCuFzokH')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09d8130ec584c6fad87bda1a68cc74a67224bab37bcf74941c3cce44e422d7e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
