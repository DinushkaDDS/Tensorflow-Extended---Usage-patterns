{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Beam and Airflow for Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've been looking at building ML pipeline componets separately and how they would interconnect. Now we should look at how we can put all those components together and how to run a full pipeline. To do that we will mainly look at 2 orchestors Apache Airflow and Beam.\n",
    "\n",
    "The importance of this type of orchestration tool is that otherwise we would need to write code to check when a component complete its task and trigger the other etc. (earlier we used interactive pipelines, but we cant use those in production systems.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Beam\n",
    "\n",
    "TFX comes with a beam version by default. Therefore for a minimal setup using beam as orchastration tool is a valid choice. It is straight forward to setup and use while allowing to use existing data processing pipelines like google cloud dataflow.\n",
    "But it lacks tools for scheduling model updates or monitoring the pipeline job progresses.\n",
    "\n",
    "\n",
    "### Apache Airflow\n",
    "\n",
    "Apache Airflow is already a widely used tool for data-loading task. It provide support to connect with production ready databases like PostgreSQL and execute partial pipelines etc which can save significant amount of time.\n",
    "\n",
    "\n",
    "### Kubeflow pipelines\n",
    "\n",
    "If there's an already existing kubernetes pipeline then it would make sense to use a Kubeflow pipelines tool. But it is complicated to setup compared to other tools. On the other hand it opens up opportunities to view TFDV and TFMA visializations, model lineage and artifact collections.\n",
    "\n",
    "\n",
    "With those details in mind, we can now look in to converting our interactive code to a script which can be used to automate the whole process.\n",
    "\n",
    "First we will define a function to initialize all the component required by our pipeline. This helps us to during the configuration of pipelines to setup for different orchastrators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For demonstration only. DO NOT RUN!\n",
    "\n",
    "\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx.components import (CsvExampleGen, Evaluator, ExampleValidator, Pusher,\n",
    "                            ResolverNode, SchemaGen, StatisticsGen, Trainer,\n",
    "                            Transform)\n",
    "from tfx.components.base import executor_spec\n",
    "from tfx.components.trainer.executor import GenericExecutor\n",
    "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
    "from tfx.proto import pusher_pb2, trainer_pb2\n",
    "from tfx.types import Channel\n",
    "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
    "\n",
    "\n",
    "def init_components(data_dir, module_file, serving_model_dir, training_steps, eval_steps):\n",
    "\n",
    "\n",
    "    example_gen = CsvExampleGen(...)\n",
    "    statistics_gen = StatisticsGen(...)\n",
    "    schema_gen = SchemaGen(...)\n",
    "    validator = ExampleValidator(...)\n",
    "    transformer = Transform(...)\n",
    "    trainer = Trainer(...)\n",
    "    model_resolver = ResolverNode(...)\n",
    "    eval_config = tfma.EvalConfig(...)\n",
    "    evaluator = Evaluator(...)\n",
    "    pusher = Pusher(...)\n",
    "\n",
    "    components = [\n",
    "        example_gen, statistics_gen, schema_gen, \n",
    "        validator, transformer, trainer, \n",
    "        model_resolver, evaluator, pusher\n",
    "    ]\n",
    "    return components\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this function we need to provide the inputs such as data location, where the model should be saved, module files required by the trainer and transform components along with hyperparameters.\n",
    "\n",
    "\n",
    "When we are converting a simple interactive pipeline to a beam or airflow version we can directly use our jupyter notebooks. For any cells we dont need to export use the cell magic `%%skip_for_export`. Then we can setup the rest as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN! Dummy Code segment\n",
    "\n",
    "orchester_type = 'beam'  # or airflow\n",
    "pipeline_name  = 'test_pipeline_beam'\n",
    "\n",
    "notebook_file = 'notebook/file/location'\n",
    "\n",
    "#pipeline inputs\n",
    "data_dir = 'data/stored/path'\n",
    "module_file = 'module/file/stored/path'\n",
    "requirement_file = 'requirement/config/file/path'\n",
    "\n",
    "output_base_path = 'base/output/path'\n",
    "serving_model_dir = 'model/save/dir/path'\n",
    "pipeline_root = 'pipeline/base/path'\n",
    "metadata_path = 'data/store/path'\n",
    "\n",
    "pipeline_export_file = 'pipline/script/export/path'\n",
    "\n",
    "# From interactive context of TFX run this\n",
    "context.export_to_pipeline(notebook_file_path=notebook_file,\n",
    "                           export_file_path=pipeline_export_file,\n",
    "                           runner_type=orchester_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code will generate a python script that can be run using Apache beam or Airflow directly. Else we can go through the method descriobed below as well.\n",
    "\n",
    "### Orchestrating using Apache Beam\n",
    "\n",
    "Apache beam is not complex nor feature rich as airflow or kubeblow. But it is easy to use and is a good method to troubleshoot/debug our works. \n",
    "\n",
    "Below we have defined a Beam Pipeline which accept the TFX pipeline components as an argument and also connects to the SQLite database as the metadata store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import absl # used for advanced logging. https://abseil.io/docs/python/guides/logging\n",
    "from tfx.orchestration import metadata, pipeline\n",
    "\n",
    "def init_beam_pipe(components, pipeline_root, direct_num_workers):\n",
    "\n",
    "    absl.logging.info(f'Pipeline root is set to {pipeline_root}.')\n",
    "\n",
    "    beam_args = [f'--direct_num_workers={direct_num_workers}', \n",
    "                 f'--requirements_file={requirement_file}']\n",
    "\n",
    "    p = pipeline.Pipeline(\n",
    "        pipeline_name=pipeline_name,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=components,\n",
    "        enable_cache=False, # If this is true some components will run using cached data\n",
    "        metadata_connection_config=metadata.sqlite_metadata_connection_config(metadata_path),\n",
    "        beam_pipeline_args=beam_args\n",
    "    )\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
    "from scripts.tfx_pipeline import init_components\n",
    "\n",
    "components = init_components(...)\n",
    "\n",
    "pipeline = init_beam_pipe(components, pipeline_root, direct_num_workers=2)\n",
    "BeamDagRunner().run(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like that we can define a beam pipeline and run. Above is not a concrete code sample. But outline the gist of what needs to be done. Also we can scale up this pipeline using `Apache Flink` if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orchesrating using Airflow\n",
    "\n",
    "Apache Airflow project is used for workflow automation. It lets us represent workflows tasks as DAGs and represent via python code. Also it lets us to schedule and monitor workflows as well(which was not available in apache beam).\n",
    "\n",
    "More details are in the [Documentation](https://airflow.apache.org/docs/apache-airflow/stable/index.html).\n",
    "\n",
    "\n",
    "We can install airflow using below. Beware of the package dependency issues.\n",
    "\n",
    "> Airflow is designed to work in linux. So will cause problems when using in windows.\n",
    "\n",
    "<center>\n",
    "\n",
    "`pip install apache-airflow`\n",
    "</center>\n",
    "\n",
    "Before using airflow we need to initialize its database. To do that we can run `airflow db init` command.\n",
    "Then we need to start the schedular which coordinates the tasks and task dependencies, as well as a webserver which provides a UI to start/stop and monitor. Also we need to start the web server to access the airflow interface. The exact procedure may change, and commands may change with time. Check the documentation.\n",
    "\n",
    "In apache airflow, it expect the workflow pipeline scripts to be in defined location. In general workflow would be something like below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "proj_config = {\n",
    "    'owner': 'Dilan Dinushka',\n",
    "    'email': 'iam@here.com',\n",
    "    'email_on_failure': True,\n",
    "    'start_date': datetime(2021, 08, 18),\n",
    "    'retires': 1,\n",
    "    'retry_delay': timedelta(days=1)\n",
    "}\n",
    "\n",
    "dag = DAG('basic_pipeline', \n",
    "            default_args=proj_config, \n",
    "            schedule_interval=timedelta(days=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it is done, we can create the workflow tasks and add to the DAG. Airflow provides bash/python task operators. An example is like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "def example_task(_id, **kwargs):\n",
    "    print(f'task {_id}')\n",
    "    return f'completed task {_id}'\n",
    "\n",
    "task_1 = PythonOperator(\n",
    "    task_id='task_1',\n",
    "    provide_context = True,\n",
    "    python_callable=example_task,\n",
    "    op_kwargs={'_id':1},\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "task_2 = PythonOperator(\n",
    "    task_id='task_2',\n",
    "    provide_context = True,\n",
    "    python_callable=example_task,\n",
    "    op_kwargs={'_id':2},\n",
    "    dag=dag\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But in TFX we dont need to do this as framework does the work for us. But in any case if we need to do something different above is an example way.\n",
    "\n",
    "Important point to note in above workflows is some tasks depend on more than one components. Therefore it is important to make such dependencies explicitly to make our pipeline run in the desired way.\n",
    "\n",
    "We can do that by setting the downstream tasks or airflow specified notation like below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_1.set_downstream(task_2)\n",
    "\n",
    "# BOTH THESE ARE EQUIVALENT\n",
    "\n",
    "task_1 >> task_2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With those concepts in mind we can now move into using airflow with TFX. TFX provides a class AirflowDAGRunner, to help us build the pipeline very similar to using beam pipelines with bit more configurations.\n",
    "\n",
    "First we create the TFX pipeline in normal way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.orchestration import metadata, pipeline\n",
    "from typing import Text\n",
    "\n",
    "airflow_config = {\n",
    "    'schedule_interval': None,\n",
    "    'start_date': datetime.datetime(2022, 08, 18),\n",
    "    'pipeline_name': 'your_ml_pipeline',\n",
    "}\n",
    "\n",
    "def init_pipeline(components, pipeline_root:Text,\n",
    "                  direct_num_workers:int) -> pipeline.Pipeline:\n",
    "    beam_arg = [\n",
    "        \"--direct_num_workers={}\".format(direct_num_workers),\n",
    "    ]\n",
    "    p = pipeline.Pipeline(pipeline_name=pipeline_name,\n",
    "                          pipeline_root=pipeline_root,\n",
    "                          components=components,\n",
    "                          enable_cache=True,\n",
    "                          metadata_connection_config=metadata.\n",
    "                          sqlite_metadata_connection_config(metadata_path),\n",
    "                          beam_pipeline_args=beam_arg)\n",
    "    return p\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialize the Airflow DAG and execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN. NOT COMPLETE!\n",
    "\n",
    "from tfx.orchestration.airflow.airflow_dag_runner import AirflowDagRunner, AirflowPipelineConfig\n",
    "from scripts.tfx_pipeline import init_components\n",
    "\n",
    "\n",
    "components = init_components(...)\n",
    "pipeline = init_pipeline(...)\n",
    "DAG = AirflowDagRunner(AirflowPipelineConfig(airflow_config)).run(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a TFX pipeline in airflow and we can do the triggering an monitoring of it using the the provided GUI in the web server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Machine_Learning_Pipelines-cCuFzokH')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09d8130ec584c6fad87bda1a68cc74a67224bab37bcf74941c3cce44e422d7e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
