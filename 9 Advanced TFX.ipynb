{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced TFX\n",
    "\n",
    "> I HAVE NOT FULLY TESTED THE CODE SEGMENTS IN THIS NOTEBOOK AND I AM SURE THAT CURRENT TFX DOCUMENTATION PROVIDE BETTER APPROACHES TO DO THE SAME THINGS!\n",
    "\n",
    "> This notebook just points the topics that we should be aware of. Not actual working implementations.\n",
    "\n",
    "As we saw most of the simpler ML works can be done using generic pipeline components provided by the TFX. But in some cases we need to go beyond the generic component flows and should have complex compoents. Here we are focusing on techniques we can use in such scenarios.\n",
    "\n",
    "\n",
    "### Training multiple models simultaneously\n",
    "\n",
    "In many production systems, it if often required to train more than one model. And TFX provide support to train several models using a single pipeline. In such cases all the data preparation/ validation and transformation steps remains the same, but from there onwards data may flow into different types of models. We can do this by defining several trainer components in the TFX pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_trainer(module_file, instance_name,\n",
    "                train_steps=5000, eval_steps=100):\n",
    "    return Trainer(\n",
    "        module_file=module_file,\n",
    "        custom_executor_spec=executor_spec.ExecutorClassSpec(\n",
    "            GenericExecutor),\n",
    "        examples=transform.outputs['transformed_examples'],\n",
    "        transform_graph=transform.outputs['transform_graph'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        train_args=trainer_pb2.TrainArgs(num_steps=train_steps),\n",
    "        eval_args=trainer_pb2.EvalArgs(num_steps=eval_steps),\n",
    "        instance_name=instance_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like using the above rough code segment, we can define different trainer components with different module files inside the pipeline. By doing this we can branch out and build complex ML pipelines than a single path pipeline.\n",
    "\n",
    "<center><image src=\"imgs/6.jpg\" width=\"500\"/></center>\n",
    "\n",
    "### TFLite model Exporting\n",
    "\n",
    "Mobile application development is one of the most major task in today world software engineering tasks. When deploying ML models to such applications we have problems such as low computational power, low storage, restrained power usage etc. To use in such cases we can use TFLite models. We can convert our exising models as a part of our pipeline to TFLite models. But it should be noted that, not all TF operations are supported by the TFLite models. So should expect problems in converting problems if model uses complicated operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm Start Model Training\n",
    "\n",
    "Not to get confused with transfer learning. In tranfter learning we repurpose the weights learned to a different task. Instead in warm-start we train a model from a previous checkpoint and continue. This is extreamely useful in training large models and also to comply with GDPR like regulations.\n",
    "\n",
    "### Human in the Loop\n",
    "\n",
    "In some ML usecases, there are requirements to model to be checked by a human in the middle of the pipeline. Once this review is done model can be sent in to the next parts of the ML cycle and TFX provides experimental support to complete such requirements. \n",
    "\n",
    "For example, tfx provides a Slack component which will send a message in a slack channel notifying data scientist to review the model. Then data scientists can go and check the model with What if tool, validate edge cases etc and give model the blessing if its okay.\n",
    "\n",
    "But basically speaking we can define our own custom components to connect to necessary channel to provide human in the loop like configuration.\n",
    "\n",
    "\n",
    "### Custom TFX components\n",
    "\n",
    "IMO custom TFX components are really important to get functionalities that would otherwise be hard to achieve using the provided components. Some example usages of custom components are,\n",
    "\n",
    "- ingesting data from a custom database\n",
    "- notifying responsible teams after a step in the pipeline(devops, daa science teams)\n",
    "- triggering custom model deployment pipes\n",
    "- tracking custom information of the ml pipeline\n",
    "\n",
    "If we want to write a custom component, we need to implement few component pieces (spec, driver and executor). First we need to define the inputs and outputs of our component as a Component Spec. Then we can define the component executor which includes the logic for how the input data should be processed to required outputs. If we need additional data(inputs) that is not available in the metadata store, we may need to write a custom component driver as well.\n",
    "\n",
    "For the demonstration we will consider a component, which will ingest jpeg images and its labels to the pipeline.\n",
    "\n",
    "Before process the data we need to define the component specification, the inputs we are expecting and outputs we will send out (also need to define the parameters as well). These inputs are called channels in terms of TFX pipeline and pipeline components communicate through them. Below is a sample code segment for our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.types.component_spec import ChannelParameter\n",
    "from tfx.types.component_spec import ExecutionParameter\n",
    "from tfx.types import standard_artifacts\n",
    "from tfx import types\n",
    "from typing import Any, Dict, List, Text\n",
    "\n",
    "\n",
    "\n",
    "class ImageIngestComponentSpec(types.ComponentSpec):\n",
    "\n",
    "    PARAMETERS = {\n",
    "        'name': ExecutionParameter(type=Text)\n",
    "    }\n",
    "\n",
    "    INPUTS = {\n",
    "        'input': ChannelParameter(type=standard_artifacts.ExternalArtifact)\n",
    "    }\n",
    "\n",
    "    OUTPUTS = {\n",
    "        'examples': ChannelParameter(type=standard_artifacts.Examples)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have defined 2 Channel parameters one for getting inputs as a path and other for storing the processed data(TFRecords). Also we are providing an additional parameter as name.\n",
    "\n",
    "In above example we have used 2 types of `Channels` (the term used in TFX for defined inputs and outputs). Based on the usecase we might need to change the type of Channel for our inputs and outputs for example to customtrainer component we might need Example type channel as an input and ExampleStatistics as an output. Below are some of the available channel types we can use.\n",
    "\n",
    "- ExampleStatistics                     \n",
    "- Model\n",
    "- ModelBlessing                         \n",
    "- Bytes\n",
    "- String                                \n",
    "- Integer\n",
    "- Float                                 \n",
    "- HyperParameter\n",
    "\n",
    "Once we have the spec we can define the executor part of our custom component. In this, we need to define a Do function which will be used by TFX for the execution. Below is the sample implementation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components.base import base_executor\n",
    "\n",
    "class ImageIngestComponentExecutor(base_executor.BaseExecutor):\n",
    "\n",
    "    def Do(self, input_dict: Dict[Text, List[types.Artifact]],\n",
    "                 output_dict: Dict[Text, List[types.Artifact]],\n",
    "                 exec_properties: Dict[Text, Any]) -> None:\n",
    "\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Do` function expects 3 arguments namely input_dict, output_dict and exec_properties. These are python dictionaries and contain artifact references and execution properties we pass along. \n",
    "\n",
    "Before implementing the complete function, we will first write a dummy function to mimic reading images from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "def _bytes_features(value):\n",
    "    if isinstance(value, str):\n",
    "        value = value.encode('utf8')\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_features(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def img_to_tfrecord(file_name, tf_writer, input_base_uri):\n",
    "\n",
    "    '''dummy function to read image files from a given location'''\n",
    "\n",
    "    image_path = os.path.join(input_base_uri, file_name)\n",
    "\n",
    "    # We assume that last part of the image name contain its label.\n",
    "    label = image_path.split()[1]\n",
    "\n",
    "    raw_file = tf.io.read_file(image_path) \n",
    "    example = tf.train.Example(features=tf.train.Features(feature={ \n",
    "        'image_raw': _bytes_features(raw_file.numpy()),\n",
    "        'label': _int64_features(label)\n",
    "    }))\n",
    "    tf_writer.write(example.SerializeToString())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the reading function in place, we can work on the actual implementation of the data ingestion component. It should be noted that reading files in batched need to be handled here. For simplicity below code does not include that. But ideally it should come as a execution parameter of the ComponentSpec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.types import artifact_utils\n",
    "\n",
    "class ImageIngestComponentExecutor(base_executor.BaseExecutor):\n",
    "\n",
    "    def Do(self, input_dict: Dict[Text, List[types.Artifact]],\n",
    "                 output_dict: Dict[Text, List[types.Artifact]],\n",
    "                 exec_properties: Dict[Text, Any]) -> None:\n",
    "\n",
    "        # Logging the starting of the function using the parent function\n",
    "        self._log_startup(input_dict, output_dict, exec_properties)\n",
    "\n",
    "        # Reading the `input` as we defined in the ComponentSpec\n",
    "        input_base_uri = input_dict['input']\n",
    "        image_files = tf.io.gfile.listdir(input_base_uri)\n",
    "\n",
    "        # Spliting the data, This should be parameterized ideally\n",
    "        train_images, eval_images = image_files[100:], image_files[:100]\n",
    "        splits = [('train', train_images), ('eval', eval_images)]\n",
    "        \n",
    "        \n",
    "        for split_name, images in splits:\n",
    "            \n",
    "            # Get the desired output dir details from the arguments.\n",
    "            output_dir = artifact_utils.get_split_uri(\n",
    "                output_dict['examples'], split_name)\n",
    "            \n",
    "\n",
    "            tf.io.gfile.mkdir(output_dir)\n",
    "            tfrecords_filename = os.path.join(output_dir, 'images.tfrecords')\n",
    "          \n",
    "            options = tf.io.TFRecordOptions(compression_type=None)\n",
    "            tf_writer = tf.io.TFRecordWriter(tfrecords_filename, options=options)\n",
    "\n",
    "            for image_filename in images:\n",
    "                example = img_to_tfrecord(image_filename, tf_writer, input_base_uri)\n",
    "                tf_writer.write(example.SerializeToString())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFX artifact_utils function provides various helper functions to get data from artifact dictionaries. \n",
    "\n",
    "Once we have the executor, we need to make sure it comply with the TFX. To do that we need to register our component inputs with the metadata store so that later components can identify the custom inputs. This is where custom drivers comes into play.\n",
    "\n",
    "> It should be noted that custom drivers not very common to use. Usually we can reuse a existing TFX component's input/output architecture or if inputs are already registered with metadata store this is not needed.\n",
    "\n",
    "We can use BaseDriver class provided by TFX to write a custom driver. There we need to overwrite the reslove_input_artifacts method. Here we need to use publish_artifacts function to register each of our inputs to the metadata store. Below is a sample implementation of such function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I HAVE NOT TESTED THIS CODE TO FUNCTION\n",
    "\n",
    "from tfx.components.base import base_driver\n",
    "from tfx.types import channel_utils\n",
    "\n",
    "class ImageIngestDriver(base_driver.BaseDriver):\n",
    "\n",
    "  \"\"\"Custom driver for registering the image inputs.\"\"\"\n",
    "\n",
    "  def resolve_input_artifacts(\n",
    "                    self,\n",
    "                    input_channels: Dict[Text, types.Channel],\n",
    "                    exec_properties: Dict[Text, Any],\n",
    "                    driver_args,\n",
    "                    pipeline_info) -> Dict[Text, List[types.Artifact]]:\n",
    "\n",
    "    \"\"\"Overrides BaseDriver.resolve_input_artifacts().\"\"\"\n",
    "    \n",
    "    # Deleting unused values\n",
    "    del driver_args \n",
    "    del pipeline_info\n",
    "\n",
    "    input_dict = channel_utils.unwrap_channel_dict(input_channels) \n",
    "    for input_list in input_dict.values():\n",
    "        for single_input in input_list:\n",
    "\n",
    "            # Publish the artifacts to the MEtadata store\n",
    "            self._metadata_handler.publish_artifacts([single_input]) \n",
    "    return input_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it should be noted in current TFX documentation they provide different and easy approaches. Also I have not tested this code, so better referring the documentation. \n",
    "\n",
    "[Tensorflow Extended Custom Components](tensorflow.org/tfx/tutorials/tfx/python_function_component#custom_python_function_components)\n",
    "\n",
    "[Understanding Custom Components](https://www.tensorflow.org/tfx/guide/understanding_custom_components)\n",
    "\n",
    "\n",
    "Anyhow, once we have all the required components we can assemble the custom component we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components.base import base_component\n",
    "from tfx import types\n",
    "from tfx.types import channel_utils\n",
    "from tfx.components.base import base_component, executor_spec\n",
    "\n",
    "class ImageIngestComponent(base_component.BaseComponent):\n",
    "\n",
    "    SPEC_CLASS = ImageIngestComponentSpec\n",
    "    EXECUTOR_SPEC = executor_spec.ExecutorClassSpec(ImageIngestComponentExecutor)\n",
    "    DRIVER_CLASS = ImageIngestDriver\n",
    "\n",
    "    def __init__(self, input, output_data=None, name=None):\n",
    "\n",
    "        if not output_data:\n",
    "            examples_artifact = standard_artifacts.Examples()\n",
    "\n",
    "            # This is not best practise. For demonstration only.\n",
    "            examples_artifact.split_names = artifact_utils.encode_split_names(['train', 'eval'])\n",
    "\n",
    "            output_data = channel_utils.as_channel([examples_artifact])\n",
    "\n",
    "        spec = ImageIngestComponentSpec(input=input,\n",
    "                                        examples=output_data,\n",
    "                                        name=name)\n",
    "        super(ImageIngestComponent, self).__init__(spec=spec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we define our custom component we can use it like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "\n",
    "context = InteractiveContext()\n",
    "image_file_path = \"/path/to/files\"\n",
    "example_gen = ImageIngestComponent(input=image_file_path,\n",
    "                                   name=u'ImageIngestComponent')\n",
    "context.run(example_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So above code segments outline the basics of the custom components. But its not usable in production systems. There are no dynamic splitting (hardcoded), lots of boilerplate codes and not scalable. Therefore it is much better to reuse existing components and build upon them to perform our custom tasks.\n",
    "\n",
    "This way we can use the functionality provided by apache beam pipelines and can build components with minimum amount of coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Machine_Learning_Pipelines-cCuFzokH')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09d8130ec584c6fad87bda1a68cc74a67224bab37bcf74941c3cce44e422d7e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
