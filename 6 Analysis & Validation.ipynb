{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing and Validating ML models\n",
    "\n",
    "After the model training, most important part we need to do is its analysis and validation. This helps us to improve the model and to identify issues in the model we build. To help with this in production systems TFX provides Tensorflow Model Analysis (TFMA). \n",
    "\n",
    "Model analysis starts with our choice of metrics. Based on our requirement we need to choose our metrics and evaluate them accordingly.\n",
    "\n",
    "These metrics include Precision, recall, F1 score, Mean Absolute Error, Maximum absolute percentage error, Mean squred error etc. (these are provided by TFMA)\n",
    "\n",
    "In TFX pipeline, TFMA calculates metrics(We define) based on the saved model that is exported by the Trainer component. If we are using Tensorboard, we will only get approximations extrapolated from measurements on mini batches. But TFMA calculates measurements on the whole evaluation set.\n",
    "\n",
    "<center>\n",
    "\n",
    "\n",
    "**pip install tensorflow-model-analysis**\n",
    "</center>\n",
    "\n",
    "For the model analysis task with TFMA, it expects 2 inputs a saved model and a evaluation set. Below is an example usage of TFMA for out previously built model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow as tf\n",
    "\n",
    "# stop tf warnings \n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "eval_model = tfma.default_eval_shared_model(eval_saved_model_path='data/tfx/Trainer/model/6/Format-Serving',\n",
    "                                                    tags=[tf.saved_model.SERVING])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing anything, we need to tell TFMA what we need to measure and provide specifications if theres any and importantly target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.protobuf import text_format\n",
    "\n",
    "# Setup tfma.EvalConfig settings\n",
    "eval_config = text_format.Parse(\"\"\"\n",
    "                ## Model information\n",
    "                model_specs {\n",
    "                    # For keras (and serving models) we need to add a `label_key`.\n",
    "                    label_key: \"consumer_disputed\"\n",
    "                }\n",
    "\n",
    "                metrics_specs {\n",
    "                    metrics { class_name: \"BinaryAccuracy\" }\n",
    "                    metrics { class_name: \"Precision\" }\n",
    "                    metrics { class_name: \"Recall\" }\n",
    "                    metrics { class_name: \"ExampleCount\" }\n",
    "                    metrics { class_name: \"FalsePositives\" }\n",
    "                    metrics { class_name: \"TruePositives\" }\n",
    "                    metrics { class_name: \"FalseNegatives\" }\n",
    "                    metrics { class_name: \"TrueNegatives\" }\n",
    "                }\n",
    "\n",
    "                ## Slicing information\n",
    "                slicing_specs {}  # overall slice\n",
    "                \n",
    "                \"\"\", tfma.EvalConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Tensorflow version (2.9.1) found. Note that TFMA support for TF 2.0 is currently in beta\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
     ]
    }
   ],
   "source": [
    "eval_result = tfma.run_model_analysis(\n",
    "    eval_shared_model=eval_model,\n",
    "    eval_config=eval_config,\n",
    "    data_location='data/eval_inputs/data_tfrecord-00000-of-00001',\n",
    "    output_path='data/eval_outputs',\n",
    "    file_format='tfrecords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that TFMA rendering on jupyter notebooks require special commands to run. Please refer the [Documentation](https://www.tensorflow.org/tfx/model_analysis/install)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee479a5e78db42f381479c28d4f4660e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SlicingMetricsViewer(config={'weightedExamplesColumn': 'example_count'}, data=[{'slice': 'Overall', 'metrics':…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfma.view.render_slicing_metrics(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above include some examples for the usage. More details can be found easily through the documentation. \n",
    "\n",
    "One main concern in many commercial productionalize ML systems is fairness. This covers issues related to races, genders etc. that could make negative impact on both ML system and the user. Therefore we should recognize such problems earlier and fix them.\n",
    "\n",
    "To do that, we can use the slicing option given in the TFMA. It helps us to separate groups we are interested in and them check the measures on those slices.\n",
    "\n",
    "For example, below we have defined a slice speciication on products column in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Tensorflow version (2.9.1) found. Note that TFMA support for TF 2.0 is currently in beta\n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n",
      "WARNING:apache_beam.io.filebasedsink:Deleting 1 existing files in target path matching: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0b64c2334f4faca1cf0208d9311d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SlicingMetricsViewer(config={'weightedExamplesColumn': 'example_count'}, data=[{'slice': 'Overall', 'metrics':…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "slice = [tfma.slicer.SingleSliceSpec(),  # This returns a slicer which return the whole dataset.\n",
    "         tfma.slicer.SingleSliceSpec(columns=['product'])]\n",
    "\n",
    "eval_result = tfma.run_model_analysis(\n",
    "    eval_shared_model=eval_model,\n",
    "    eval_config=eval_config,\n",
    "    data_location='data/eval_inputs/data_tfrecord-00000-of-00001',\n",
    "    output_path='data/eval_outputs',\n",
    "    file_format='tfrecords',\n",
    "    slice_spec=slice)\n",
    "\n",
    "tfma.view.render_slicing_metrics(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Fairness indicators for decisions\n",
    "\n",
    "Fairness indicators is a useful tool for model analysis which has overlapping capabilities with TFMA. Its ability to view matrices sliced on featues at various decision thresholds helps to identify the model fairness at different levels of thresholds.\n",
    "\n",
    "There are severalways to use fairness indicator tool and one way is using with tensorboard. To do that we need to install the plugin first using below.\n",
    "\n",
    "<center> \n",
    "\n",
    "`pip install tensorboard_plugin_fairness_indicators`\n",
    "</center>\n",
    "\n",
    "Next we can use TFMA to evaluate the model and calculate metrics for a set of decision threshold we supply. This is supplied to TFMA in metrics_spec argument for the Eval_Config. Below is an example for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_config_fairness=tfma.EvalConfig(\n",
    "        model_specs=[tfma.ModelSpec(label_key='consumer_disputed')],\n",
    "        slicing_specs=[tfma.SlicingSpec(), tfma.SlicingSpec(feature_keys=['product'])],\n",
    "        metrics_specs=[\n",
    "              tfma.MetricsSpec(metrics=[\n",
    "                  tfma.MetricConfig(class_name='BinaryAccuracy'),\n",
    "                  tfma.MetricConfig(class_name='ExampleCount'),\n",
    "                  tfma.MetricConfig(class_name='FalsePositives'),\n",
    "                  tfma.MetricConfig(class_name='TruePositives'),\n",
    "                  tfma.MetricConfig(class_name='FalseNegatives'),\n",
    "                  tfma.MetricConfig(class_name='TrueNegatives'),\n",
    "                  tfma.MetricConfig(class_name='FairnessIndicators', config='{\"thresholds\":[0.25, 0.5, 0.75]}')\n",
    "              ])])\n",
    "\n",
    "eval_result = tfma.run_model_analysis(\n",
    "    eval_shared_model=eval_model,\n",
    "    eval_config=eval_config_fairness,\n",
    "    data_location='data/eval_inputs/data_tfrecord-00000-of-00001',\n",
    "    output_path=\"./data/eval_outputs/\",\n",
    "    file_format='tfrecords',\n",
    "    slice_spec = slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write the evaluation results so that it can be used later like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=data/eval_logs/fairness_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What-If Tool\n",
    "\n",
    "This weird named tool, which exactly does what it means, can show how individual data points were affected by the model. It provides features to do extra visualizations other than TFMA and investigate individual data points.\n",
    "\n",
    "There are several ways to use the what if tool and below is on such method.\n",
    "First we need to install it by below.\n",
    "\n",
    "<center> \n",
    "\n",
    "`pip install witwidget`\n",
    "</center>\n",
    "\n",
    "Then we need to load the data as a TFRecordDatase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = tf.data.TFRecordDataset('data/eval_inputs/data_tfrecord-00000-of-00001')\n",
    "subset = eval_data.take(1000)\n",
    "eval_examples = [tf.train.Example.FromString(d.numpy()) for d in subset]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to load the model and define a prediction function that takes list of TFExamples and returns predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = tf.saved_model.load(export_dir='data/tfx/Trainer/model/6/Format-Serving')\n",
    "predict_fn = model.signatures['serving_default']\n",
    "\n",
    "\n",
    "def predict(examples):\n",
    "    test_examples = tf.constant([example.SerializeToString() for example in examples])\n",
    "    preds = predict_fn(examples=test_examples)\n",
    "    return preds['outputs'].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the WIT configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from witwidget.notebook.visualization import WitConfigBuilder\n",
    "\n",
    "config_builder = WitConfigBuilder(eval_examples).set_custom_predict_fn(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we visualize the data with our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3923bf231efa4465b50f3634edba25cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "WitWidget(config={'model_type': 'classification', 'label_vocab': [], 'are_sequence_examples': False, 'inferenc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from witwidget.notebook.visualization import WitWidget\n",
    "WitWidget(config_builder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if tool provides many interesting features to identifying various model behaviours such as `counterfactuals` and `partial dependency plots` etc. More details about what if tool can be found in the [documentation](https://pair-code.github.io/what-if-tool/index.html).\n",
    "\n",
    "Also WIT can be used for model explainability tasks as well. We can use the features provided by WIT such as counterfactuals, PDPs to explain model behaviour in various situations. \n",
    "\n",
    "Other than that we can use techniques such as LIME, SHAPLEY values to obtain model explainability aspects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and Validation in TFX\n",
    "\n",
    "All the above mentioned techniques include ways to identify models with good qualities. But in production environments we need to automate the process of this identification. To do that, TFX provides Resolver, Evaluator and Pusher components. These components can check model performance on a evaluation dataset and send the model to serving phase if its performance is better.\n",
    "\n",
    "> TFX uses a concept called `blessing` to describe the gating process for deciding whether or not to deploy a model.\n",
    "\n",
    "### Resolver\n",
    "\n",
    "If we need to compare a new model against a previous version resolver can be used. This checks the TFX metadata store and send the latest best performing model as a baseline to the evaluator so we can compare it with the new model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluator\n",
    "\n",
    "This uses TFMA library to evaluate model predictions on a validation set. It takes input from ExampleGen component, trained model from Trainer component and EvalConfig for TFMA.\n",
    "\n",
    "### Pusher\n",
    "\n",
    "It takes a saved model as an input, file path for the model saving location and based on the configuration check the model has been blessed by th Evaluator (better compared to the baseline). If thats the case new model will be pushed to the serving location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Machine_Learning_Pipelines-cCuFzokH')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09d8130ec584c6fad87bda1a68cc74a67224bab37bcf74941c3cce44e422d7e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
